{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebb39bda-a85b-4423-9017-cfac1fb87b7a",
   "metadata": {},
   "source": [
    "# Assignment 3 : Neural Volume Rendering and Surface Rendering\n",
    "\n",
    "#### Submitted by: Shahram Najam Syed\n",
    "#### Andrew-ID: snsyed\n",
    "#### Date: 11th March, 2025\n",
    "#### Late days used: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca54213-c4dd-4e71-a666-bb1d25f9479b",
   "metadata": {},
   "source": [
    "## A. Neural Volume Rendering (80 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96296f6c-22c7-4b9d-8c7c-3773a0b45976",
   "metadata": {},
   "source": [
    "### 0. Transmittance Calculation (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6315af-07f4-4d81-a116-6d0126c78b64",
   "metadata": {},
   "source": [
    "<img src=\"./output/figure1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b62356e-2c77-4e54-bf6b-573a2c75e585",
   "metadata": {},
   "source": [
    "Since, \n",
    "$$\n",
    "\\frac{dT}{dy} = -\\sigma(y)T\n",
    "$$\n",
    "\n",
    "Hence the base equation for transmittance becomes:\n",
    "$$\n",
    "T = e^{-\\int \\sigma(y) dy}\n",
    "$$\n",
    "\n",
    "So,\n",
    "$$\n",
    "T(y_1, y_2) = e^{-\\int_{y_1}^{y_2} \\sigma(y) dy} = e^{-2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "T(y_2, y_4) = e^{-\\int_{y_2}^{y_3} \\sigma(y) dy} \\times e^{-\\int_{y_3}^{y_4} \\sigma(y) dy} = e^{-30.5}\n",
    "$$\n",
    "\n",
    "$$\n",
    "T(x, y_4) = T(x, y_2) \\times T(y_2, y_4) = T(x, y_1) \\times T(y_1, y_2) \\times T(y_2, y_4) = e^{-32.5}\n",
    "$$\n",
    "\n",
    "$$\n",
    "T(x, y_3) = T(x, y_1) \\times T(y_1, y_2) \\times T(y_2, y_3) = e^{-2.5}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7c6e27-8dee-48bb-b142-30b8a41be423",
   "metadata": {},
   "source": [
    "### 1. Differentiable Volume Rendering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e267cb1-ca1d-4172-aacc-e9c42dc07ddf",
   "metadata": {},
   "source": [
    "#### 1.3. Ray sampling (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb0b11b-f17c-4421-b8b0-57aee3b6aa50",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th>Grid Visualization</th>\n",
    "<th>Ray Visualization</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"./images/1_3_xygrid.png\"></td>\n",
    "<td><img src=\"./images/1_3_rays.png\"></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c39eccd-92d2-441a-aea0-9fcd780ad8d1",
   "metadata": {},
   "source": [
    "### 1.3. Fitting a mesh (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f59fd9-2e1b-43b7-96e1-9d6151db7178",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th>Source (reconstructed)</th>\n",
    "<th>Target (goal)</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"./output_loop/q1_3_1.gif\"></td>\n",
    "<td><img src=\"./output_loop/q1_3_2.gif\"></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf16b29-22bc-41bf-a958-0ddaff7bd9e2",
   "metadata": {},
   "source": [
    "## 2. Reconstructing 3D from single view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef62afdb-c2ae-47c1-8d60-23c6efae25b7",
   "metadata": {},
   "source": [
    "### 2.1. Image to voxel grid (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ff639b-fd28-495f-ab8c-83d7fffe8ed8",
   "metadata": {},
   "source": [
    "Voxels proved to be a challenging representation to train, especially when using a pix2vox‐inspired decoder. The **Baseline FC + Deconv Decoder** first passed the 512‑dimensional feature through a fully connected layer, reshaping it into a small 3D grid, which was then upsampled via transposed 3D convolutions to form a full 32×32×32 volume. Despite numerous adjustments—such as modifying the number of layers and initial volume size—this approach often resulted in a high rate of empty meshes (about 29%) and low average F1 scores (around 27).\n",
    "\n",
    "To address these issues, I experimented with simpler architectures based solely on fully connected layers. A deeper **5‑layer FC network** offered some improvement, reducing the mesh failure rate to roughly 10% and boosting the F1 score to 42.3. Ultimately, the **Simplified FC Network (3 layers)** with an iso threshold of 0.5 delivered the best performance, with 3% failures and an average F1 score of 62.3.\n",
    "\n",
    "Detailed abalation of these networks can be seen in the table below:\n",
    "\n",
    "| Architecture Variant             | Iso Threshold | Mesh Failure Rate | Avg F1 Score @ 0.05 |\n",
    "|----------------------------------|---------------|-------------------|---------------------|\n",
    "| Baseline FC + Deconv Decoder     | 0.3           | ~29%              | 27.6                |\n",
    "| Deep FC Network (5 Layers)       | 0.5           | ~10%              | 42.3                |\n",
    "| Simplified FC Network (3 Layers) | 0.5           | 3%                | 62.1                |\n",
    "\n",
    "These results indicate that reducing the architectural complexity—by relying on a more straightforward FC-based design—can lead to more stable training and better reconstruction quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c15467f-55e6-425f-bd75-60f87b27433e",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th>Input (RGB)</th>\n",
    "<th>GT mesh</th>\n",
    "<th>Predicted 3D voxel grid</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"./output/q2_1_1.png\" width=\"256\" height=\"256\"></td>\n",
    "<td><img src=\"./output/q2_1_2.gif\" width=\"256\" height=\"256\"></td>\n",
    "<td><img src=\"./output/q2_1_3.gif\" width=\"256\" height=\"256\"></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"./output/q2_1_4.png\" width=\"256\" height=\"256\"></td>\n",
    "<td><img src=\"./output/q2_1_5.gif\" width=\"256\" height=\"256\"></td>\n",
    "<td><img src=\"./output/q2_1_6.gif\" width=\"256\" height=\"256\"></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"./output/q2_1_7.png\" width=\"256\" height=\"256\"></td>\n",
    "<td><img src=\"./output/q2_1_8.gif\" width=\"256\" height=\"256\"></td>\n",
    "<td><img src=\"./output/q2_1_9.gif\" width=\"256\" height=\"256\"></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58b6bce-44af-4ea4-9dcd-a6b47f4200ac",
   "metadata": {},
   "source": [
    "### 2.2. Image to point cloud (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a31a2b7-e3d6-4a07-be99-8a423f79d5b3",
   "metadata": {},
   "source": [
    "Point cloud reconstruction from a single image benefits from a relatively straightforward decoder architecture. The baseline approach employs a simple fully connected (FC) network that regresses the 3D coordinates for each point directly from the 512‑dimensional image encoding. Initially, using a single FC layer with a standard ReLU activation led to some instability, with the predicted point distributions lacking coherence. By introducing a two‐layer FC network with a LeakyReLU followed by a Tanh activation, the network was able to generate more consistent and accurate point clouds. In our ablation studies, the optimized two‐layer variant achieved the highest average F1 score at the evaluation threshold.\n",
    "\n",
    "| Architecture Variant              | Activation Function         | Avg F1 Score @ 0.05 |\n",
    "|-----------------------------------|-----------------------------|---------------------|\n",
    "| Baseline Single FC                | ReLU                        | 35.9                |\n",
    "| Two-Layer FC with LeakyReLU       | LeakyReLU, Tanh             | 72.2                |\n",
    "| Optimized Two-Layer FC            | LeakyReLU followed by Tanh  | 83.8                |\n",
    "\n",
    "The experiments show that a carefully designed FC decoder—using non-linearities that balance expressiveness and stability—can significantly improve point cloud quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f89347-0ca3-43ea-bce5-0b5a1c948130",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th>Input (RGB)</th>\n",
    "<th>GT mesh</th>\n",
    "<th>Predicted 3D Points</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"./output/q2_2_1.png\" width=\"256\" height=\"256\"></td>\n",
    "<td><img src=\"./output_loop/q2_2_2.gif\" width=\"256\" height=\"256\"></td>\n",
    "<td><img src=\"./output_loop/q2_2_3.gif\" width=\"256\" height=\"256\"></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"./output/q2_2_4.png\" width=\"256\" height=\"256\"></td>\n",
    "<td><img src=\"./output_loop/q2_2_5.gif\" width=\"256\" height=\"256\"></td>\n",
    "<td><img src=\"./output_loop/q2_2_6.gif\" width=\"256\" height=\"256\"></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"./output/q2_2_7.png\" width=\"256\" height=\"256\"></td>\n",
    "<td><img src=\"./output_loop/q2_2_8.gif\" width=\"256\" height=\"256\"></td>\n",
    "<td><img src=\"./output_loop/q2_2_9.gif\" width=\"256\" height=\"256\"></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9389f3-577e-4d83-87d3-0624df0329c5",
   "metadata": {},
   "source": [
    "### 2.3 Image to mesh (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283ba388-741a-4504-98b6-c23dc876b308",
   "metadata": {},
   "source": [
    "For mesh reconstruction, the challenge lies in accurately deforming an initial template mesh (typically an icosphere) to match the target geometry. The baseline mesh decoder starts with an icosphere generated at a moderate resolution (Level 4) and directly regresses vertex offsets using a single FC layer with a Tanh activation. Although this setup captures the coarse structure, it often restricts the deformation range, leading to suboptimal mesh quality. By increasing the network depth to a two- or three-layer FC decoder, the model can better capture finer geometric details and allow for more extensive vertex adjustments. In our ablation experiments, the three-layer FC mesh decoder provided the best balance, producing high-fidelity reconstructions with minimal deviation from the ground truth.\n",
    "\n",
    "| Architecture Variant               | Mesh Initialization | Avg F1 Score @ 0.05 |\n",
    "|------------------------------------|---------------------|---------------------|\n",
    "| Baseline Single FC Mesh Decoder    | Icosphere Level 4   | 36.4                |\n",
    "| Two-Layer FC Mesh Decoder          | Icosphere Level 4   | 68.7                |\n",
    "| Three-Layer FC Mesh Decoder        | Icosphere Level 4   | 81.1                |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84af37f6-a583-41ce-bf3b-d532faea3a15",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th>Input (RGB)</th>\n",
    "<th>GT mesh</th>\n",
    "<th>Predicted Mesh</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"./output/q2_3_1.png\" width=\"256\" height=\"256\"></td>\n",
    "<td><img src=\"./output_loop/q2_3_2.gif\" width=\"256\" height=\"256\"></td>\n",
    "<td><img src=\"./output_loop/q2_3_3.gif\" width=\"256\" height=\"256\"></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"./output/q2_3_4.png\" width=\"256\" height=\"256\"></td>\n",
    "<td><img src=\"./output_loop/q2_3_5.gif\" width=\"256\" height=\"256\"></td>\n",
    "<td><img src=\"./output_loop/q2_3_6.gif\" width=\"256\" height=\"256\"></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"./output/q2_3_7.png\" width=\"256\" height=\"256\"></td>\n",
    "<td><img src=\"./output_loop/q2_3_8.gif\" width=\"256\" height=\"256\"></td>\n",
    "<td><img src=\"./output_loop/q2_3_9.gif\" width=\"256\" height=\"256\"></td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214cc4e6-3006-4e98-9412-c68b9fa6a0e4",
   "metadata": {},
   "source": [
    "### 2.4. Quantitative comparisions(10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0609f9da-aec5-4e33-bf9b-1d0408806ee8",
   "metadata": {},
   "source": [
    "In my experiments, I observed a similar trend across the three representations:\n",
    "\n",
    "- **Point Clouds:**  \n",
    "  This approach achieved the highest F1 score. Its strength lies in its flexibility—points are free to occupy any position in 3D space, allowing the network to capture detailed geometric nuances when enough points are used.\n",
    "\n",
    "- **Meshes:**  \n",
    "  Mesh reconstruction, which involves deforming an icosphere, performed moderately well. However, its performance is constrained by the fixed number of vertices in the initial sphere template, limiting the ability to capture fine details compared to the point cloud approach.\n",
    "\n",
    "- **Voxels:**  \n",
    "  Voxels, represented by a 32³ grid, delivered the lowest F1 score. The coarse resolution makes it difficult to capture intricate details. Additionally, the binary cross-entropy loss applied in this setting might not offer as informative a signal as the Chamfer distance loss used for point clouds and meshes.\n",
    "\n",
    "Overall, these results confirm that point cloud representations tend to capture object geometry more accurately, followed by meshes and then voxels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b6b1e3-1cf3-41ea-bc97-b7bd3f38d5f9",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th>Voxel</th>\n",
    "<th>Point</th>\n",
    "<th>Mesh</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"./output/q2_4_1.png\"></td>\n",
    "<td><img src=\"./output/q2_4_2.png\"></td>\n",
    "<td><img src=\"./output/q2_4_3.png\"></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6670cda-1c44-4419-a141-21ed80ca5b4a",
   "metadata": {},
   "source": [
    "### 2.5. Analyse effects of hyperparams variations (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e06a30-89c8-4b07-9d29-e90739166c8f",
   "metadata": {},
   "source": [
    "I conducted an ablation study on the smoothing weight (`w_smooth`), while keeping the Chamfer distance weight (`w_chamfer`) constant at 1.0. In this experiment, I evaluated three settings: 0, 0.5, and 10.0.\n",
    "\n",
    "#### Qualitative Observations\n",
    "\n",
    "- **No Smoothing (w_smooth = 0):**  \n",
    "  The reconstructed mesh maintains its sharp features, which can result in a somewhat pointy appearance. However, without any smoothing, the model may not regularize the surface as effectively.\n",
    "\n",
    "- **Moderate Smoothing (w_smooth = 0.5):**  \n",
    "  Introducing a moderate amount of smoothing yields visually smoother surfaces, reducing pointiness. The improvement in visual quality is noticeable even if the F1 score does not change drastically.\n",
    "\n",
    "- **Excessive Smoothing (w_smooth = 10.0):**  \n",
    "  When the smoothing weight is set too high, the performance drops. Over-smoothing appears to wash out critical geometric details, leading to less accurate reconstructions.\n",
    "\n",
    "#### Quantitative Results\n",
    "\n",
    "The F1 scores observed under each setting were as follows:\n",
    "\n",
    "| w_smooth | F1 Score |\n",
    "|----------|----------|\n",
    "| 0        | 81.1     |\n",
    "| 0.5      | 75.9     |\n",
    "| 10.0     | 69.4     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b587147b-1716-4bd5-a82f-2b6d4cd5cce7",
   "metadata": {},
   "source": [
    "<style>\n",
    "  img {\n",
    "    width: 256px;\n",
    "    height: 256px;\n",
    "    object-fit: cover; /* Ensures uniform scaling */\n",
    "  }\n",
    "</style>\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th>Input (RGB)</th>\n",
    "<th>GT mesh</th>\n",
    "<th>w_smooth = 0.0</th>\n",
    "<th>w_smooth = 0.5</th>\n",
    "<th>w_smooth = 10.0</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"./output/q2_5_1.png\"></td>\n",
    "<td><img src=\"./output_loop/q2_5_2.gif\"></td>\n",
    "<td><img src=\"./output_loop/q2_5_13.gif\"></td>\n",
    "<td><img src=\"./output_loop/q2_5_14.gif\"></td>\n",
    "<td><img src=\"./output_loop/q2_5_15.gif\"></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b982f917-c17e-456f-b639-13c098de7e9b",
   "metadata": {},
   "source": [
    "### 2.6. Interpret your model (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c065f595-3472-48be-a1ee-8b2a66f68cf4",
   "metadata": {},
   "source": [
    "To evaluate the robustness of the trained point cloud model, I intentionally introduced artifacts into one of the training images. Specifically, I applied both horizontal and vertical occlusions to simulate challenging, pseudo out-of-distribution scenarios. Remarkably, the network maintained strong classification performance despite these perturbations, demonstrating its resilience to input distortions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb336ec-6380-48a3-b161-2fe6abbbf57c",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th>Input OOD (RGB)</th>\n",
    "<th>Output Point</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"./output/q2_6_1.png\" width=\"256\" height=\"256\"></td>\n",
    "<td><img src=\"./output_loop/q2_6_2.gif\" width=\"256\" height=\"256\"></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"./output/q2_6_3.png\" width=\"256\" height=\"256\"></td>\n",
    "<td><img src=\"./output_loop/q2_6_4.gif\" width=\"256\" height=\"256\"></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"./output/q2_6_5.png\" width=\"256\" height=\"256\"></td>\n",
    "<td><img src=\"./output_loop/q2_6_6.gif\" width=\"256\" height=\"256\"></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"./output/q2_6_7.png\" width=\"256\" height=\"256\"></td>\n",
    "<td><img src=\"./output_loop/q2_6_8.gif\" width=\"256\" height=\"256\"></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba71b53-1c9f-465b-99da-debec7e142af",
   "metadata": {},
   "source": [
    "## 3. Exploring other architectures / datasets. (Choose at least one! More than one is extra credit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dfeec0-4896-4f72-9873-8308dcaa0e00",
   "metadata": {},
   "source": [
    "### 3.1 Implicit network (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbddd1a-efe4-4c6e-a0cb-6e12989950ea",
   "metadata": {},
   "source": [
    "For the implicit network, I trained the model to predict occupancy values on a pre-generated 32×32×32 grid of 3D coordinates. The model takes as input both the image feature and the 3D grid coordinate (x, y, z), which are concatenated and passed through multiple layers of a fully connected network. Specifically, the decoder is a multi-layer perceptron (MLP) with hidden layers that progressively reduce in size. Each hidden layer employs a ReLU activation, while the final output layer uses a Sigmoid function to produce occupancy values between 0 and 1, indicating the presence or absence of an object at each grid point. The model was trained using a loss function similar to the voxel-based approach, typically binary cross-entropy. Although the implicit network exhibited slower convergence rates compared to the voxel model and achieved a lower F1 score of 62.7, its flexibility allows it to adapt to different voxel structures, making it an interesting alternative despite the longer training time.\n",
    "\n",
    "In our implicit network, we adopt a coordinate-based approach for 3D reconstruction that differs from the fixed-grid voxel method. We first pre-generate a 32×32×32 grid of 3D coordinates and then, for each coordinate, concatenate it with the image feature vector extracted from the input image. This combined vector is passed through a multi-layer perceptron (MLP) with several fully connected layers using ReLU activations, and finally through a Sigmoid-activated output layer to predict an occupancy value between 0 and 1. The training uses the same binary cross-entropy loss as the voxel model. Although the implicit model converges more slowly due to its increased flexibility and complexity, it ultimately achieves an F1 score of 62.7, demonstrating its potential to handle varied voxel structures effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26baeef-a58a-43e7-8b22-3a5cf50bea4a",
   "metadata": {},
   "source": [
    "<center>\n",
    "  <img src=\"./output/q3_1_10.png\">\n",
    "</center>\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th>Input (RGB)</th>\n",
    "<th>Output Predicted Voxel</th>\n",
    "</tr>\n",
    "<!--tr>\n",
    "<td><img src=\"./output/q3_1_1.png\" width=\"256\" height=\"256\"></td>\n",
    "<td><img src=\"./output_loop/q3_1_3.gif\" width=\"256\" height=\"256\"></td>\n",
    "</tr-->\n",
    "<!--tr>\n",
    "<td><img src=\"./output/q3_1_4.png\" width=\"256\" height=\"256\"></td>\n",
    "<td><img src=\"./output_loop/q3_1_6.gif\" width=\"256\" height=\"256\"></td>\n",
    "</tr-->\n",
    "<tr>\n",
    "<td><img src=\"./output/q3_1_7.png\" width=\"256\" height=\"256\"></td>\n",
    "<td><img src=\"./output_loop/q3_1_9.gif\" width=\"256\" height=\"256\"></td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538ac466-c1e3-4811-9e7a-2402e4bc4aa7",
   "metadata": {},
   "source": [
    "### 3.2 Parametric network (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af6591a-8749-4bf9-8cd6-a1fe12d29b4b",
   "metadata": {},
   "source": [
    "In our parametric network, we sample 2000 points from a 2D unit circle and concatenate each (x, y) coordinate with the image feature vector. This combined input is fed into an MLP—several fully connected layers with ReLU activations—that regresses a corresponding 3D point. Training uses the same loss as the point-cloud model. Although convergence is slower and performance slightly lower due to time constraints, the model achieves an F1 score of 65.2 while offering flexibility to handle an arbitrary number of points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507e43f3-6ad3-46e9-9b6b-b66efe8b7783",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th>Input (RGB)</th>\n",
    "<th>Output Predicted Points</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"./output/q3_2_1.png\" width=\"256\" height=\"256\"></td>\n",
    "<td><img src=\"./output_loop/q3_2_2.gif\" width=\"256\" height=\"256\"></td>\n",
    "</tr>\n",
    "<!--tr>\n",
    "<td><img src=\"./output/q3_2_3.png\" width=\"256\" height=\"256\"></td>\n",
    "<td><img src=\"./output_loop/q3_2_4.gif\" width=\"256\" height=\"256\"></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"./output/q3_2_5.png\" width=\"256\" height=\"256\"></td>\n",
    "<td><img src=\"./output_loop/q3_2_6.gif\" width=\"256\" height=\"256\"></td>\n",
    "</tr-->\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed00ca4-3660-4ff4-8a1c-5b20c56740f1",
   "metadata": {},
   "source": [
    "### 3.3 Extended dataset for training (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60ec2b0-fec5-40a0-a27e-6e1f93d5e4a4",
   "metadata": {},
   "source": [
    "Extending the dataset improved the model’s ability to generalize for a multiclass problem, although the qualitative results degraded. Training on three classes and testing on just the chair class achieved an F1 score of 95.9%, while training on one class and testing on the chair class yielded 83.8%. When evaluating on all three classes, the F1 score was 84.1% for the model trained on three classes, and surprisingly, 79.8% for the model trained on a single class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122d4e99-aa82-40d9-9ab0-6cd3a510e91c",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th> </th>\n",
    "<th>Training on 3 classes</th>\n",
    "<th>Training on 1 class</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><b>Tested on 1 class</b></td>\n",
    "<td><img src=\"./output/q3_3_1.png\" width=\"512\" height=\"512\"></td>\n",
    "<td><img src=\"./output/q3_3_2.png\" width=\"512\" height=\"512\"></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><b>Tested on 3 classes</b></td>\n",
    "<td><img src=\"./output/q3_3_4.png\" width=\"512\" height=\"512\"></td>\n",
    "<td><img src=\"./output/q3_3_3.png\" width=\"512\" height=\"512\"></td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf12606-9dd1-4b89-83fb-18f1dc18d417",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th> </th>\n",
    "<th>Input RGB</th>\n",
    "<th>Training on 3 classes</th>\n",
    "<th>Training on 1 class</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><b>Aeroplane class</b></td>\n",
    "<td><img src=\"./output/q3_3_5.png\" width=\"512\" height=\"512\"></td>\n",
    "<td><img src=\"./output_loop/q3_3_6.gif\" width=\"512\" height=\"512\"></td>\n",
    "<td><img src=\"./output_loop/q3_3_7.gif\" width=\"512\" height=\"512\"></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><b>Chair class</b></td>\n",
    "<td><img src=\"./output/q3_3_8.png\" width=\"512\" height=\"512\"></td>\n",
    "<td><img src=\"./output_loop/q3_3_9.gif\" width=\"512\" height=\"512\"></td>\n",
    "<td><img src=\"./output_loop/q3_3_10.gif\" width=\"512\" height=\"512\"></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><b>Car class</b></td>\n",
    "<td><img src=\"./output/q3_3_11.png\" width=\"512\" height=\"512\"></td>\n",
    "<td><img src=\"./output_loop/q3_3_12.gif\" width=\"512\" height=\"512\"></td>\n",
    "<td><img src=\"./output_loop/q3_3_13.gif\" width=\"512\" height=\"512\"></td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81639073-c30a-4b67-8a7d-57842d0d4f52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
