{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebb39bda-a85b-4423-9017-cfac1fb87b7a",
   "metadata": {},
   "source": [
    "# Assignment 3 : Neural Volume Rendering and Surface Rendering\n",
    "\n",
    "#### Submitted by: Shahram Najam Syed\n",
    "#### Andrew-ID: snsyed\n",
    "#### Date: 11th March, 2025\n",
    "#### Late days used: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca54213-c4dd-4e71-a666-bb1d25f9479b",
   "metadata": {},
   "source": [
    "## A. Neural Volume Rendering (80 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96296f6c-22c7-4b9d-8c7c-3773a0b45976",
   "metadata": {},
   "source": [
    "### 0. Transmittance Calculation (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6315af-07f4-4d81-a116-6d0126c78b64",
   "metadata": {},
   "source": [
    "<img src=\"./output/figure1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b62356e-2c77-4e54-bf6b-573a2c75e585",
   "metadata": {},
   "source": [
    "Since, \n",
    "$$\n",
    "\\frac{dT}{dy} = -\\sigma(y)T\n",
    "$$\n",
    "\n",
    "Hence the base equation for transmittance becomes:\n",
    "$$\n",
    "T = e^{-\\int \\sigma(y) dy}\n",
    "$$\n",
    "\n",
    "So,\n",
    "$$\n",
    "T(y_1, y_2) = e^{-\\int_{y_1}^{y_2} \\sigma(y) dy} = e^{-2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "T(y_2, y_4) = e^{-\\int_{y_2}^{y_3} \\sigma(y) dy} \\times e^{-\\int_{y_3}^{y_4} \\sigma(y) dy} = e^{-30.5}\n",
    "$$\n",
    "\n",
    "$$\n",
    "T(x, y_4) = T(x, y_2) \\times T(y_2, y_4) = T(x, y_1) \\times T(y_1, y_2) \\times T(y_2, y_4) = e^{-32.5}\n",
    "$$\n",
    "\n",
    "$$\n",
    "T(x, y_3) = T(x, y_1) \\times T(y_1, y_2) \\times T(y_2, y_3) = e^{-2.5}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bd6321-b20a-48a1-bf40-4ab95a13bb97",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7c6e27-8dee-48bb-b142-30b8a41be423",
   "metadata": {},
   "source": [
    "### 1. Differentiable Volume Rendering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e267cb1-ca1d-4172-aacc-e9c42dc07ddf",
   "metadata": {},
   "source": [
    "#### 1.3. Ray sampling (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb0b11b-f17c-4421-b8b0-57aee3b6aa50",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th>Grid Visualization</th>\n",
    "<th>Ray Visualization</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"./images/1_3_xygrid.png\"></td>\n",
    "<td><img src=\"./images/1_3_rays.png\"></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225e902d-8861-49d1-81b6-eab992df5a71",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c39eccd-92d2-441a-aea0-9fcd780ad8d1",
   "metadata": {},
   "source": [
    "#### 1.4. Point sampling (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f59fd9-2e1b-43b7-96e1-9d6151db7178",
   "metadata": {},
   "source": [
    "<center><img src=\"./images/1_4_pts_sampled.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61f6bdf-d4bb-49ea-aaf8-1dc84048ca8c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42575448-7288-4866-9079-ebc4104f945a",
   "metadata": {},
   "source": [
    "#### 1.5. Volume rendering (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38b62b9-136f-40e2-9938-0c18ff3a5ed4",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th>Color Visualization</th>\n",
    "<th>Depth Visualization</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"./images/part_1.gif\"></td>\n",
    "<td><img src=\"./images/1_5_depth.png\"></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf17a98f-6a3d-4b0d-a073-c39406bdbc0b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf16b29-22bc-41bf-a958-0ddaff7bd9e2",
   "metadata": {},
   "source": [
    "### 2. Optimizing a basic implicit volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef62afdb-c2ae-47c1-8d60-23c6efae25b7",
   "metadata": {},
   "source": [
    "#### 2.1. Random ray sampling (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ff639b-fd28-495f-ab8c-83d7fffe8ed8",
   "metadata": {},
   "source": [
    "Implemented in ray_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf8930e-ef5b-4c74-976a-b89667d27cd9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58b6bce-44af-4ea4-9dcd-a6b47f4200ac",
   "metadata": {},
   "source": [
    "#### 2.2. Loss and training (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a31a2b7-e3d6-4a07-be99-8a423f79d5b3",
   "metadata": {},
   "source": [
    "* Box side length: (2.02, 1.50, 1.50)\n",
    "* Center: (0.25, 0.25, 0.00)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8278f220-273f-46c9-8259-0b1699ce8fb3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9389f3-577e-4d83-87d3-0624df0329c5",
   "metadata": {},
   "source": [
    "#### 2.3. Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84af37f6-a583-41ce-bf3b-d532faea3a15",
   "metadata": {},
   "source": [
    "<center><img src=\"./images/part_2.gif\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbe6e85-3f9b-4b11-a70f-2655cfa48365",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214cc4e6-3006-4e98-9412-c68b9fa6a0e4",
   "metadata": {},
   "source": [
    "### 3. Optimizing a Neural Radiance Field (NeRF) (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b6b1e3-1cf3-41ea-bc97-b7bd3f38d5f9",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th>Epochs=10</th>\n",
    "<th>Epochs=50</th>\n",
    "<th>Epochs=100</th>\n",
    "<th>Epochs=250</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"./images/part_epoch_10.gif\"></td>\n",
    "<td><img src=\"./images/part_epoch_50.gif\"></td>\n",
    "<td><img src=\"./images/part_epoch_100.gif\"></td>\n",
    "<td><img src=\"./images/part_epoch_250.gif\"></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f69b3d1-6c57-4e4c-968f-7d2521751f44",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6670cda-1c44-4419-a141-21ed80ca5b4a",
   "metadata": {},
   "source": [
    "### 4. NeRF Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2812053c-7255-4289-90a5-abc9603bb4bc",
   "metadata": {},
   "source": [
    "#### 4.1 View Dependence (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6c7638-a8d4-4509-866e-3df695daf708",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th>Low-res with no view dependence</th>\n",
    "<th>Low-res with view dependence</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"./images/part_4_1_lr_nvd.gif\"></td>\n",
    "<td><img src=\"./images/part_4_1_lr_vd.gif\"></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfa1d15-c7d3-4808-894f-6abde10a3f5d",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th>High-res with no view dependence</th>\n",
    "<th>High-res with view dependence</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"./images/part_4_1_hr_nvd.gif\"></td>\n",
    "<td><img src=\"./images/part_4_1_hr_vd.gif\"></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efccdcb6-0566-4784-97aa-cca75a71d1a7",
   "metadata": {},
   "source": [
    "**Observation:** Juxtaposing low-resolution no view dependence vs view dependence I don't observe a marked difference for the subtle lighting variations and specular highlights, which makes sense due to limited resolution. On the flip side, the subtle variations is much evident for high-resolution renders.\n",
    "\n",
    "**Trade-offs b/w view dependence vs generalization quality:**\n",
    "* View-dependent method captures intricatre scene details by leveraging viewpoint specific information, yielding photorealistic renderings that generalized approaches may lack.\n",
    "* View dependent method for ample resolution (as observed) produce crisper, high-fidelity, and more defined results due to their ability to adapt rendering to specific viewing angles.\n",
    "* But, view-dependent models struggle with novel unseen viewpoints, increasing the risk of voerfitting to training data. In contrast, generalized methods prioritize robustness across diverse angles.\n",
    "* The added details comes at a cost requiring greater model complexity leading to higher computational and memory expenses compared to lightweight and parameter-efficient generalized approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db999730-82cd-4967-99cf-65378c7ea84b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a60e8c-bc32-4828-bbd7-36b6a63f037e",
   "metadata": {},
   "source": [
    "#### 4.2 Coarse/Fine Sampling (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c6a8dd-95c7-412f-94f8-7323861bb2f2",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th>Scene</th>\n",
    "<th>Before Coarse/Fine Sampling</th>\n",
    "<th>After Coarse/Fine Sampling</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Materials</td>\n",
    "<td><img src=\"./images/4_2_wo_cfs.gif\"></td>\n",
    "<td><img src=\"./images/4_2_w_cfs.gif\"></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Lego</td>\n",
    "<td><img src=\"./images/4_2_wo_cfs_lego.gif\"></td>\n",
    "<td><img src=\"./images/4_2_w_cfs_lego.gif\"></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e06a30-89c8-4b07-9d29-e90739166c8f",
   "metadata": {},
   "source": [
    "**Trade-offs (Speed vs. Quality):**\n",
    "\n",
    "* Quality Improvement: Fine sampling captures high-frequency details (e.g., textures, thin structures), yielding photorealistic renders, while the coarse pass avoids \"wasting\" samples on empty regions, refining only relevant areas.\n",
    "\n",
    "* Speed Cost: Two-pass sampling doubles computation per ray (coarse + fine network queries). And additionally, training/inference time increases, but total samples per ray remain fixed (e.g., 128 total = 64 coarse + 64 fine)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da62b8a-6258-4042-bacf-306b4b1baab3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7dc294-7539-42d6-a007-d08487b119f5",
   "metadata": {},
   "source": [
    "## B. Neural Surface Rendering (50 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f45885f-c63c-490a-a1f3-c313145d5799",
   "metadata": {},
   "source": [
    "### 5. Sphere Tracing (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56f9dff-8c0d-44af-87aa-02e4a5eb70f0",
   "metadata": {},
   "source": [
    "<center><img src=\"./images/part_5.gif\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed87d58f-d502-4074-bbec-f300cd22310f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ed86d2-37c3-40e5-8e98-9905406a75c8",
   "metadata": {},
   "source": [
    "### 6. Optimizing a Neural SDF (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871d1682-85a9-4b66-85c8-dc2662f50128",
   "metadata": {},
   "source": [
    "For the Neural SDF implementation, I designed an MLP that effectively learns to predict signed distance values for any input point in 3D space. Here's a breakdown of the architecture:\n",
    "\n",
    "* Positional Encoding\n",
    "    - The input 3D coordinates are transformed using a harmonic embedding with 4 frequencies.\n",
    "    - This transformation helps the network capture high-frequency details, which is crucial for representing fine surface details in the SDF.\n",
    "\n",
    "* Skip Connection Network\n",
    "    - A deep MLP with 6 layers and 128 neurons per hidden layer.\n",
    "    - Skip connections directly feed the input encoding to intermediate layers, significantly improving gradient flow during training and helping the network learn complex surfaces.\n",
    "\n",
    "* Final Output Layer\n",
    "    - Unlike density fields in NeRF that require non-negative outputs, SDF values can be positive (outside the surface), negative (inside the surface), or zero (exactly on the surface).\n",
    "    - The final layer produces direct SDF values without any activation function.\n",
    "\n",
    "\n",
    "The key insight behind training an effective Neural SDF is the use of eikonal regularization, which is based on a fundamental property of SDFs:\n",
    "\n",
    "> The gradient of a proper signed distance function should have unit norm almost everywhere in space.\n",
    "\n",
    "To enforce this constraint, an eikonal loss function is implemented to penalize deviations from a unit gradient norm:\n",
    "\n",
    "```python\n",
    "def eikonal_loss(gradients):\n",
    "    gradient_norms = torch.norm(gradients, dim=-1)\n",
    "    return torch.mean(torch.square(gradient_norms - 1.0))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121334df-0ccc-4116-a683-d1e3aa4a2c7f",
   "metadata": {},
   "source": [
    "The model was trained for 5000 epochs with a learning rate of 0.0001, which gradually decreased using a scheduler (with gamma = 0.8 and step size = 50) to allow fine-tuning of the surface representation. \n",
    "\n",
    "**Hyperparameter experiments:**\n",
    "* **No. of epochs:** I observed that after 5000, increasing the number of epochs had marginal gain to offer.\n",
    "* **Eikonal loss weight:** I observed that increasing the Eikonal loss weight distorts the reconstruction while a smaller weight values tend to lose parts of the reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fdd9e6-66af-45ea-8da2-b6ae40586248",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th>Input pointcloud</th>\n",
    "<th>Epochs=100</th>\n",
    "<th>Epochs=500</th>\n",
    "<th>Epochs=1000</th>\n",
    "<th>Epochs=5000</th>\n",
    "<th>Epochs=10000</th>\n",
    "<th>Epochs=15000</th>    \n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"./images/part_6_input.gif\"></td>\n",
    "<td><img src=\"./images/part_6_100.gif\"></td>\n",
    "<td><img src=\"./images/part_6_500.gif\"></td>\n",
    "<td><img src=\"./images/part_6_1000.gif\"></td>\n",
    "<td><img src=\"./images/part_6_5000.gif\"></td>\n",
    "<td><img src=\"./images/part_6_10000.gif\"></td>\n",
    "<td><img src=\"./images/part_6_15000.gif\"></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6c32cc-d14e-4984-aafb-d7a57690baab",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th>Input pointcloud</th>\n",
    "<th>w=0.025</th>\n",
    "<th>w=0.1</th>\n",
    "<th>w=0.5</th>\n",
    "<th>w=1.0</th>\n",
    "<th>w=5.0</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"./images/part_6_input.gif\"></td>\n",
    "<td><img src=\"./images/part_6_iw_1.gif\"></td>\n",
    "<td><img src=\"./images/part_6_iw_2.gif\"></td>\n",
    "<td><img src=\"./images/part_6_iw_3.gif\"></td>\n",
    "<td><img src=\"./images/part_6_iw_4.gif\"></td>\n",
    "<td><img src=\"./images/part_6_iw_5.gif\"></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a984a43-da7e-4fb0-9f8b-82e8e46fdbaf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12c4219-61e6-4589-89a4-5784da3ce1c9",
   "metadata": {},
   "source": [
    "### 7. VolSDF (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9659c067-987f-4ad8-8e1c-e4bb5f11b824",
   "metadata": {},
   "source": [
    "Following is an abalation study with the varying values for $\\alpha$ and $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bd9dbf-bf1a-4638-bf6d-430871cf659d",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th>Alpha</th>\n",
    "<th>Beta</th>\n",
    "<th>Geometry</th>\n",
    "<th>Color</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>10.0 (default)</td>\n",
    "<td>0.05 (default)</td>\n",
    "<td><img src=\"./images/part_7_geometry_a_10_b_0.05.gif\"></td>\n",
    "<td><img src=\"./images/part_7_a_10_b_0.05.gif\"></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>1.0</td>\n",
    "<td>0.05</td>\n",
    "<td><img src=\"./images/part_7_geometry_a_1_b_0.05.gif\"></td>\n",
    "<td><img src=\"./images/part_7_a_1_b_0.05.gif\"></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>100.0</td>\n",
    "<td>0.05</td>\n",
    "<td><img src=\"./images/part_7_geometry_a_100_b_0.05.gif\"></td>\n",
    "<td><img src=\"./images/part_7_a_100_b_0.05.gif\"></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>10.0</td>\n",
    "<td>0.1</td>\n",
    "<td><img src=\"./images/part_7_geometry_a_10_b_0.1.gif\"></td>\n",
    "<td><img src=\"./images/part_7_a_10_b_0.1.gif\"></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>10.0</td>\n",
    "<td>0.5</td>\n",
    "<td><img src=\"./images/part_7_geometry_a_10_b_0.5.gif\"></td>\n",
    "<td><img src=\"./images/part_7_a_10_b_0.5.gif\"></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2af735-0551-4b43-8edc-f58d25b0c834",
   "metadata": {},
   "source": [
    "#### 1. How does high beta bias your learned SDF? What about low beta?\n",
    "\n",
    "- **High beta** creates a more smoothed density function around the surface boundary, leading to:\n",
    "  - Less precise surface boundaries\n",
    "  - Smoother overall representation\n",
    "  - More volumetric-like appearance with gradual falloff\n",
    "\n",
    "- **Low beta** creates a sharp transition in density near the zero level-set of the SDF, biasing the model to learn:\n",
    "  - Crisp, well-defined surface boundaries\n",
    "  - Higher precision in surface localization\n",
    "  - More binary-like distinctions between inside and outside\n",
    "\n",
    "Mathematically, as beta approaches zero, the density function approaches a step function at the surface boundary.\n",
    "\n",
    "#### 2. Would an SDF be easier to train with volume rendering and low beta or high beta? Why?\n",
    "\n",
    "An SDF would be easier to train with volume rendering using a **higher beta** value because:\n",
    "\n",
    "- Higher beta creates smoother gradients throughout the volume space.\n",
    "- These smoother gradients provide more meaningful learning signals during backpropagation.\n",
    "- The optimization landscape becomes less steep and easier to navigate.\n",
    "- Training is more numerically stable since density changes gradually.\n",
    "\n",
    "With very **low beta** values, training often becomes unstable because:\n",
    "\n",
    "- The density function approaches a step function with near-zero gradients everywhere except right at the surface.\n",
    "- This creates **vanishing gradient** problems during optimization.\n",
    "- Small errors in the SDF can lead to large changes in rendered appearance.\n",
    "- The loss landscape becomes more rugged with many local minima.\n",
    "\n",
    "#### 3. Would you be more likely to learn an accurate surface with high beta or low beta? Why?\n",
    "\n",
    "We would likely learn a more accurate surface with a **lower beta** value (though not too low) because:\n",
    "\n",
    "- Lower beta encourages the model to precisely localize the surface boundary.\n",
    "- The sharper transition creates stronger incentives for the network to accurately model the zero level-set.\n",
    "- Fine details and sharp features are better preserved.\n",
    "- The rendered output more closely resembles the true surface geometry.\n",
    "\n",
    "##### Trade-offs:\n",
    "- **High beta** leads to overly smooth surfaces that lose detail.\n",
    "- **Too low a beta** makes training unstable and prone to getting stuck in poor local minima.\n",
    "\n",
    "The optimal approach is often a **curriculum strategy**:\n",
    "1. Start with a **higher beta** for stable initial training.\n",
    "2. Gradually **reduce beta** to refine surface details as training progresses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb664d3-ea23-4415-9188-985426d93290",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fd493a-2eff-45ea-8fbf-621236c19a7e",
   "metadata": {},
   "source": [
    "### 8. Neural Surface Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0138b1ec-cd90-4464-83a9-3ec0b07f60bf",
   "metadata": {},
   "source": [
    "#### 8.1. Render a Large Scene with Sphere Tracing (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c736c3ab-86c0-450f-848e-43a1de952c3c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8eb62370-7c12-47f8-89b2-de1cf5632f58",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01218ffe-23f1-463b-9846-de039e840e17",
   "metadata": {},
   "source": [
    "#### 8.2 Fewer Training Views (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f2d342-9a69-477a-8a96-8747f70a8392",
   "metadata": {},
   "source": [
    "##### Comparing VolSDF and NeRF with Fewer Training Views\n",
    "\n",
    "When reducing the number of training views from **100** to fewer views (like 10), several interesting differences emerge between **VolSDF** and **NeRF** approaches.\n",
    "\n",
    "* **VolSDF Performance**\n",
    "    - **With 20 views**: VolSDF maintains relatively good geometry but shows some artifacts in areas with limited observation.\n",
    "    - **With 10 views**: The geometric structure is still recognizable, but there's notable deterioration in surface quality with shadows and distortions appearing.\n",
    "\n",
    "* NeRF Performance\n",
    "    - **With 20 views**: NeRF produces visually pleasing renderings but with less geometric consistency.\n",
    "    - **With 10 views**: NeRF struggles significantly, with blurry results and phantom geometry in unobserved regions.\n",
    "\n",
    "##### Analysis of the Differences\n",
    "The key advantage of **VolSDF** in low-view settings comes from its **explicit surface representation** through the **SDF constraint**. The **eikonal regularization** enforces a proper distance field, which provides strong **geometric prior knowledge** even in regions with sparse observations. However, **NeRF** can sometimes produce **more visually appealing results** in terms of texture and color blending, even when the underlying geometry is incorrect. This is because NeRF focuses purely on **appearance matching** without geometric constraints.\n",
    "\n",
    "Overall, **VolSDF is better suited for preserving geometry in low-view scenarios**, while **NeRF can produce more visually appealing but less geometrically accurate results**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc96dc9f-641c-4e39-850e-1024a0ed46af",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th>Training Views</th>\n",
    "<th>Vol SDF (Geometric)</th>\n",
    "<th>Vol SDF (Color)</th>\n",
    "<th>NeRF</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>10</td>\n",
    "<td><img src=\"./images/part_8_2_geometry_10_views.gif\"></td>\n",
    "<td><img src=\"./images/part_8_2_10_views.gif\"></td>\n",
    "<td><img src=\"./images/part_8_2_nerf_10_views.gif\"></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>20</td>\n",
    "<td><img src=\"./images/part_8_2_geometry_50_views.gif\"></td>\n",
    "<td><img src=\"./images/part_8_2_50_views.gif\"></td>\n",
    "<td><img src=\"./images/part_8_2_nerf_50_views.gif\"></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>All views</td>\n",
    "<td><img src=\"./images/part_8_2_geometry_all_views.gif\"></td>\n",
    "<td><img src=\"./images/part_8_2_all_views.gif\"></td>\n",
    "<td><img src=\"./images/part_8_2_nerf_all_views.gif\"></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab807ce7-821f-4326-b134-658f027cb2c5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648f61c1-1b32-4e5e-9405-13a406600749",
   "metadata": {},
   "source": [
    "#### 8.3 Alternate SDF to Density Conversions (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dfc9a8-f25c-48e7-bd9e-5cdbb2f6602a",
   "metadata": {},
   "source": [
    "##### VolSDF Method\n",
    "The **VolSDF** approach uses this function:\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\alpha \\cdot \n",
    "\\begin{cases} \n",
    "1 - \\frac{1}{2} e^{d(x)/\\beta}, & \\text{if } d(x) \\leq 0 \\\\ \n",
    "\\frac{1}{2} e^{-d(x)/\\beta}, & \\text{if } d(x) > 0 \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $d(x)$ is the **SDF value**.\n",
    "- $\\alpha$ controls the **overall density magnitude**.\n",
    "- $\\beta$ controls the **transition sharpness**.\n",
    "\n",
    "##### NeuS Method\n",
    "The **NeuS** \"naive\" approach uses:\n",
    "\n",
    "$$\n",
    "\\sigma(x) = s \\cdot \\frac{e^{-s \\cdot d(x)}}{(1 + e^{-s \\cdot d(x)})^2}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $s$ is a parameter controlling **transition sharpness**.\n",
    "\n",
    "##### Key Differences\n",
    "\n",
    "* **Characteristics:**\n",
    "    - **VolSDF**: Asymmetric inside/outside behavior.\n",
    "    - **NeuS**: Symmetric around the zero level-set.\n",
    "\n",
    "* **Results:**\n",
    "    - **VolSDF** produced **smoother, more stable** results.\n",
    "    - **NeuS** created **sharper surfaces** with high $s$ values but was **less stable**.\n",
    "    - **NeuS** was **highly sensitive** to the choice of $s$.\n",
    "\n",
    "* **Trade-offs:**\n",
    "    - **Higher $s$ in NeuS** â†’ **Sharper surfaces** but increased **training instability**.\n",
    "    - **VolSDF** offered **better control** through separate $\\alpha$ and $\\beta$ parameters.\n",
    "\n",
    "##### Conclusion\n",
    "The **NeuS** approach is **mathematically elegant** (being the derivative of the sigmoid function) but requires **careful parameter tuning**, while **VolSDF proved more robust in practice**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499e0784-0674-4dca-ac09-ce8df7b9498a",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th>Geometric</th>\n",
    "    <th>Color</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>VolSDF</td>\n",
    "    <td><img src=\"./images/part_7_geometry_a_10_b_0.05.gif\" width=\"128\" height=\"128\"></td>\n",
    "    <td><img src=\"./images/part_7_a_10_b_0.05.gif\" width=\"128\" height=\"128\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>s_val = 1</td>\n",
    "    <td><img src=\"./images/part_8_3_geometry_s_1.gif\" width=\"128\" height=\"128\"></td>\n",
    "    <td><img src=\"./images/part_8_3_s_1.gif\" width=\"128\" height=\"128\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>s_val = 5</td>\n",
    "    <td><img src=\"./images/part_8_3_geometry_s_5.gif\" width=\"128\" height=\"128\"></td>\n",
    "    <td><img src=\"./images/part_8_3_s_5.gif\" width=\"128\" height=\"128\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>s_val = 50</td>\n",
    "    <td><img src=\"./images/part_8_3_geometry_s_50.gif\" width=\"128\" height=\"128\"></td>\n",
    "    <td><img src=\"./images/part_8_3_s_50.gif\" width=\"128\" height=\"128\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>s_val = 100</td>\n",
    "    <td><img src=\"./images/part_8_3_geometry_s_100.gif\" width=\"128\" height=\"128\"></td>\n",
    "    <td><img src=\"./images/part_8_3_s_100.gif\" width=\"128\" height=\"128\"></td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b98c47f-3dea-473f-917b-b381a27e7a38",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
