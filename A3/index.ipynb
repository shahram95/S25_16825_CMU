{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebb39bda-a85b-4423-9017-cfac1fb87b7a",
   "metadata": {},
   "source": [
    "# Assignment 3 : Neural Volume Rendering and Surface Rendering\n",
    "\n",
    "#### Submitted by: Shahram Najam Syed\n",
    "#### Andrew-ID: snsyed\n",
    "#### Date: 11th March, 2025\n",
    "#### Late days used: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca54213-c4dd-4e71-a666-bb1d25f9479b",
   "metadata": {},
   "source": [
    "## A. Neural Volume Rendering (80 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96296f6c-22c7-4b9d-8c7c-3773a0b45976",
   "metadata": {},
   "source": [
    "### 0. Transmittance Calculation (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6315af-07f4-4d81-a116-6d0126c78b64",
   "metadata": {},
   "source": [
    "<img src=\"./output/figure1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b62356e-2c77-4e54-bf6b-573a2c75e585",
   "metadata": {},
   "source": [
    "Since, \n",
    "$$\n",
    "\\frac{dT}{dy} = -\\sigma(y)T\n",
    "$$\n",
    "\n",
    "Hence the base equation for transmittance becomes:\n",
    "$$\n",
    "T = e^{-\\int \\sigma(y) dy}\n",
    "$$\n",
    "\n",
    "So,\n",
    "$$\n",
    "T(y_1, y_2) = e^{-\\int_{y_1}^{y_2} \\sigma(y) dy} = e^{-2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "T(y_2, y_4) = e^{-\\int_{y_2}^{y_3} \\sigma(y) dy} \\times e^{-\\int_{y_3}^{y_4} \\sigma(y) dy} = e^{-30.5}\n",
    "$$\n",
    "\n",
    "$$\n",
    "T(x, y_4) = T(x, y_2) \\times T(y_2, y_4) = T(x, y_1) \\times T(y_1, y_2) \\times T(y_2, y_4) = e^{-32.5}\n",
    "$$\n",
    "\n",
    "$$\n",
    "T(x, y_3) = T(x, y_1) \\times T(y_1, y_2) \\times T(y_2, y_3) = e^{-2.5}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7c6e27-8dee-48bb-b142-30b8a41be423",
   "metadata": {},
   "source": [
    "### 1. Differentiable Volume Rendering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e267cb1-ca1d-4172-aacc-e9c42dc07ddf",
   "metadata": {},
   "source": [
    "#### 1.3. Ray sampling (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb0b11b-f17c-4421-b8b0-57aee3b6aa50",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th>Grid Visualization</th>\n",
    "<th>Ray Visualization</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"./images/1_3_xygrid.png\"></td>\n",
    "<td><img src=\"./images/1_3_rays.png\"></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c39eccd-92d2-441a-aea0-9fcd780ad8d1",
   "metadata": {},
   "source": [
    "#### 1.4. Point sampling (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f59fd9-2e1b-43b7-96e1-9d6151db7178",
   "metadata": {},
   "source": [
    "<center><img src=\"./images/1_4_pts_sampled.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42575448-7288-4866-9079-ebc4104f945a",
   "metadata": {},
   "source": [
    "#### 1.5. Volume rendering (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38b62b9-136f-40e2-9938-0c18ff3a5ed4",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th>Color Visualization</th>\n",
    "<th>Depth Visualization</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"./images/part_1.gif\"></td>\n",
    "<td><img src=\"./images/1_5_depth.png\"></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf16b29-22bc-41bf-a958-0ddaff7bd9e2",
   "metadata": {},
   "source": [
    "### 2. Optimizing a basic implicit volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef62afdb-c2ae-47c1-8d60-23c6efae25b7",
   "metadata": {},
   "source": [
    "#### 2.1. Random ray sampling (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ff639b-fd28-495f-ab8c-83d7fffe8ed8",
   "metadata": {},
   "source": [
    "Implemented in ray_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58b6bce-44af-4ea4-9dcd-a6b47f4200ac",
   "metadata": {},
   "source": [
    "#### 2.2. Loss and training (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a31a2b7-e3d6-4a07-be99-8a423f79d5b3",
   "metadata": {},
   "source": [
    "* Box side length: (2.02, 1.50, 1.50)\n",
    "* Center: (0.25, 0.25, 0.00)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9389f3-577e-4d83-87d3-0624df0329c5",
   "metadata": {},
   "source": [
    "#### 2.3. Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84af37f6-a583-41ce-bf3b-d532faea3a15",
   "metadata": {},
   "source": [
    "<center><img src=\"./images/part_2.gif\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214cc4e6-3006-4e98-9412-c68b9fa6a0e4",
   "metadata": {},
   "source": [
    "### 3. Optimizing a Neural Radiance Field (NeRF) (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b6b1e3-1cf3-41ea-bc97-b7bd3f38d5f9",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th>Epochs=10</th>\n",
    "<th>Epochs=50</th>\n",
    "<th>Epochs=100</th>\n",
    "<th>Epochs=250</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"./images/part_epoch_10.gif\"></td>\n",
    "<td><img src=\"./images/part_epoch_50.gif\"></td>\n",
    "<td><img src=\"./images/part_epoch_100.gif\"></td>\n",
    "<td><img src=\"./images/part_epoch_250.gif\"></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6670cda-1c44-4419-a141-21ed80ca5b4a",
   "metadata": {},
   "source": [
    "### 4. NeRF Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2812053c-7255-4289-90a5-abc9603bb4bc",
   "metadata": {},
   "source": [
    "#### 4.1 View Dependence (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6c7638-a8d4-4509-866e-3df695daf708",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th>Low-res with no view dependence</th>\n",
    "<th>Low-res with view dependence</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"./images/part_4_1_lr_nvd.gif\"></td>\n",
    "<td><img src=\"./images/part_4_1_lr_vd.gif\"></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfa1d15-c7d3-4808-894f-6abde10a3f5d",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th>High-res with no view dependence</th>\n",
    "<th>High-res with view dependence</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"./images/part_4_1_hr_nvd.gif\"></td>\n",
    "<td><img src=\"./images/part_4_1_hr_vd.gif\"></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efccdcb6-0566-4784-97aa-cca75a71d1a7",
   "metadata": {},
   "source": [
    "**Observation:** Juxtaposing low-resolution no view dependence vs view dependence I don't observe a marked difference for the subtle lighting variations and specular highlights, which makes sense due to limited resolution. On the flip side, the subtle variations is much evident for high-resolution renders.\n",
    "\n",
    "**Trade-offs b/w view dependence vs generalization quality:**\n",
    "* View-dependent method captures intricatre scene details by leveraging viewpoint specific information, yielding photorealistic renderings that generalized approaches may lack.\n",
    "* View dependent method for ample resolution (as observed) produce crisper, high-fidelity, and more defined results due to their ability to adapt rendering to specific viewing angles.\n",
    "* But, view-dependent models struggle with novel unseen viewpoints, increasing the risk of voerfitting to training data. In contrast, generalized methods prioritize robustness across diverse angles.\n",
    "* The added details comes at a cost requiring greater model complexity leading to higher computational and memory expenses compared to lightweight and parameter-efficient generalized approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a60e8c-bc32-4828-bbd7-36b6a63f037e",
   "metadata": {},
   "source": [
    "#### 4.2 Coarse/Fine Sampling (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c6a8dd-95c7-412f-94f8-7323861bb2f2",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th>Scene</th>\n",
    "<th>Before Coarse/Fine Sampling</th>\n",
    "<th>After Coarse/Fine Sampling</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Materials</td>\n",
    "<td><img src=\"./images/4_2_wo_cfs.gif\"></td>\n",
    "<td><img src=\"./images/4_2_w_cfs.gif\"></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Lego</td>\n",
    "<td><img src=\"./images/4_2_wo_cfs_lego.gif\"></td>\n",
    "<td><img src=\"./images/4_2_w_cfs_lego.gif\"></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e06a30-89c8-4b07-9d29-e90739166c8f",
   "metadata": {},
   "source": [
    "**Trade-offs (Speed vs. Quality):**\n",
    "\n",
    "* Quality Improvement: Fine sampling captures high-frequency details (e.g., textures, thin structures), yielding photorealistic renders, while the coarse pass avoids \"wasting\" samples on empty regions, refining only relevant areas.\n",
    "\n",
    "* Speed Cost: Two-pass sampling doubles computation per ray (coarse + fine network queries). And additionally, training/inference time increases, but total samples per ray remain fixed (e.g., 128 total = 64 coarse + 64 fine)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7dc294-7539-42d6-a007-d08487b119f5",
   "metadata": {},
   "source": [
    "## B. Neural Surface Rendering (50 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f45885f-c63c-490a-a1f3-c313145d5799",
   "metadata": {},
   "source": [
    "### 5. Sphere Tracing (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56f9dff-8c0d-44af-87aa-02e4a5eb70f0",
   "metadata": {},
   "source": [
    "<center><img src=\"./images/part_5.gif\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ed86d2-37c3-40e5-8e98-9905406a75c8",
   "metadata": {},
   "source": [
    "### 6. Optimizing a Neural SDF (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871d1682-85a9-4b66-85c8-dc2662f50128",
   "metadata": {},
   "source": [
    "For the Neural SDF implementation, I designed an MLP that effectively learns to predict signed distance values for any input point in 3D space. Here's a breakdown of the architecture:\n",
    "\n",
    "* Positional Encoding\n",
    "- The input 3D coordinates are transformed using a harmonic embedding with 4 frequencies.\n",
    "- This transformation helps the network capture high-frequency details, which is crucial for representing fine surface details in the SDF.\n",
    "\n",
    " Skip Connection Network\n",
    "- A deep MLP with 6 layers and 128 neurons per hidden layer.\n",
    "- Skip connections directly feed the input encoding to intermediate layers, significantly improving gradient flow during training and helping the network learn complex surfaces.\n",
    "\n",
    "* Final Output Layer\n",
    "- Unlike density fields in NeRF that require non-negative outputs, SDF values can be positive (outside the surface), negative (inside the surface), or zero (exactly on the surface).\n",
    "- The final layer produces direct SDF values without any activation function.\n",
    "\n",
    "\n",
    "The key insight behind training an effective Neural SDF is the use of eikonal regularization, which is based on a fundamental property of SDFs:\n",
    "\n",
    "> The gradient of a proper signed distance function should have unit norm almost everywhere in space.\n",
    "\n",
    "To enforce this constraint, an eikonal loss function is implemented to penalize deviations from a unit gradient norm:\n",
    "\n",
    "```python\n",
    "def eikonal_loss(gradients):\n",
    "    gradient_norms = torch.norm(gradients, dim=-1)\n",
    "    return torch.mean(torch.square(gradient_norms - 1.0))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121334df-0ccc-4116-a683-d1e3aa4a2c7f",
   "metadata": {},
   "source": [
    "The model was trained for 5000 epochs with a learning rate of 0.0001, which gradually decreased using a scheduler (with gamma = 0.8 and step size = 50) to allow fine-tuning of the surface representation. \n",
    "\n",
    "**Hyperparameter experiments:**\n",
    "* No. of epochs: I observed that after 5000, increasing the number of epochs had marginal gain to offer.\n",
    "* Eikonal loss weight: I observed that increasing the Eikonal loss weight distorts the reconstruction while a smaller weight values tend to lose parts of the reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fdd9e6-66af-45ea-8da2-b6ae40586248",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th>Input pointcloud</th>\n",
    "<th>Epochs=100</th>\n",
    "<th>Epochs=500</th>\n",
    "<th>Epochs=1000</th>\n",
    "<th>Epochs=5000</th>\n",
    "<th>Epochs=10000</th>\n",
    "<th>Epochs=15000</th>    \n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"./images/part_6_input.gif\"></td>\n",
    "<td><img src=\"./images/part_6_100.gif\"></td>\n",
    "<td><img src=\"./images/part_6_500.gif\"></td>\n",
    "<td><img src=\"./images/part_6_1000.gif\"></td>\n",
    "<td><img src=\"./images/part_6_5000.gif\"></td>\n",
    "<td><img src=\"./images/part_6_10000.gif\"></td>\n",
    "<td><img src=\"./images/part_6_15000.gif\"></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6c32cc-d14e-4984-aafb-d7a57690baab",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th>Input pointcloud</th>\n",
    "<th>w=0.025</th>\n",
    "<th>w=0.1</th>\n",
    "<th>w=0.5</th>\n",
    "<th>w=1.0</th>\n",
    "<th>w=5.0</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"./images/part_6_input.gif\"></td>\n",
    "<td><img src=\"./images/part_6_iw_1.gif\"></td>\n",
    "<td><img src=\"./images/part_6_iw_2.gif\"></td>\n",
    "<td><img src=\"./images/part_6_iw_3.gif\"></td>\n",
    "<td><img src=\"./images/part_6_iw_4.gif\"></td>\n",
    "<td><img src=\"./images/part_6_iw_5.gif\"></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12c4219-61e6-4589-89a4-5784da3ce1c9",
   "metadata": {},
   "source": [
    "### 7. VolSDF (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9659c067-987f-4ad8-8e1c-e4bb5f11b824",
   "metadata": {},
   "source": [
    "Following is an abalation study with the varying values for $\\alpha$ and $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bd9dbf-bf1a-4638-bf6d-430871cf659d",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th>Alpha</th>\n",
    "<th>Beta</th>\n",
    "<th>Geometry</th>\n",
    "<th>Color</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>10.0 (default)</td>\n",
    "<td>0.05 (default)</td>\n",
    "<td><img src=\"./images/part_7_geometry_a_10_b_0.05.gif\"></td>\n",
    "<td><img src=\"./images/part_7_a_10_b_0.05.gif\"></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>1.0</td>\n",
    "<td>0.05</td>\n",
    "<td><img src=\"./images/part_7_geometry_a_1_b_0.05.gif\"></td>\n",
    "<td><img src=\"./images/part_7_a_1_b_0.05.gif\"></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>100.0</td>\n",
    "<td>0.05</td>\n",
    "<td><img src=\"./images/part_7_geometry_a_100_b_0.05.gif\"></td>\n",
    "<td><img src=\"./images/part_7_a_100_b_0.05.gif\"></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>10.0</td>\n",
    "<td>0.1</td>\n",
    "<td><img src=\"./images/part_7_geometry_a_10_b_0.1.gif\"></td>\n",
    "<td><img src=\"./images/part_7_a_10_b_0.1.gif\"></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>10.0</td>\n",
    "<td>0.5</td>\n",
    "<td><img src=\"./images/part_7_geometry_a_10_b_0.5.gif\"></td>\n",
    "<td><img src=\"./images/part_7_a_10_b_0.5.gif\"></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2829728-50b5-453a-b5a3-e5e5e697c7ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
